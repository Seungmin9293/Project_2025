import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from collections import Counter
from itertools import combinations
import networkx as nx
import warnings
import matplotlib.font_manager as fm  # <<< ìˆ˜ì •: í°íŠ¸ ê´€ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# ì‚¬ì†Œí•œ ê²½ê³  ë©”ì‹œì§€ëŠ” ë¬´ì‹œ
warnings.filterwarnings('ignore')

# --- ì‹œê°í™” ìœ„í•œ í•œê¸€ í°íŠ¸ ì„¤ì • ---
# <<< ìˆ˜ì •: ì£¼ì„ í•´ì œ ë° ì‹œìŠ¤í…œ í°íŠ¸ ìžë™ íƒìƒ‰ ë¡œì§ìœ¼ë¡œ ë³€ê²½
font_name = None
try:
    # ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ í°íŠ¸ ì¤‘ 'Malgun Gothic' ë˜ëŠ” 'Nanum' ê³„ì—´ í°íŠ¸ íƒìƒ‰
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    if 'Malgun Gothic' in available_fonts:
        font_name = 'Malgun Gothic'
    elif any('Nanum' in font for font in available_fonts):
        # 'Nanum'ì„ í¬í•¨í•˜ëŠ” í°íŠ¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ
        font_name = [font for font in available_fonts if 'Nanum' in font][0]

    if font_name:
        plt.rcParams['font.family'] = font_name
        print(f"âœ… í•œê¸€ í°íŠ¸ '{font_name}'ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ 'Malgun Gothic' ë˜ëŠ” 'Nanum' í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œê°í™” ì‹œ ê¸€ìžê°€ ê¹¨ì§ˆ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")

    plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€
except Exception as e:
    print(f"í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ---  ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬ ---
file_path = 'job_code.csv'
try:
    df = pd.read_csv(file_path, encoding='cp949')
    print(f"\n'{file_path}' íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

    df.dropna(subset=['keywords_bert'], inplace=True)
    df['keywords_list'] = df['keywords_bert'].str.split(', ')

    print("\n---------- [ ë°ì´í„° ì •ë³´ ] ----------")
    df.info()
    print("\n---------- [ ë°ì´í„° ìƒ˜í”Œ ] ----------")
    print(df.head())

except FileNotFoundError:
    print(f" ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ìœ„ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    df = pd.DataFrame()
except UnicodeDecodeError:
    print(f" ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì˜ ì¸ì½”ë”© ë¬¸ì œì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. 'utf-8'ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
        print(f"\n'{file_path}' íŒŒì¼ì„ 'utf-8'ë¡œ ë‹¤ì‹œ ì‹œë„í•˜ì—¬ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f" ì˜¤ë¥˜: íŒŒì¼ ì½ê¸°ì— ìµœì¢…ì ìœ¼ë¡œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ì¸: {e}")
        df = pd.DataFrame()

# --- ë©”ì¸ ë¶„ì„ ë¸”ë¡ (ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰) ---
if not df.empty:

    # ==========================================================================
    #  ë¶„ì„ 1: ì „ì²´ í‚¤ì›Œë“œ ë° ì§ë¬´ëª… ë¶„ì„
    # ==========================================================================
    print("\n" + "=" * 50)
    print(" ë¶„ì„ 1: ì „ì²´ í‚¤ì›Œë“œ ë° ì§ë¬´ëª… ë¶„ì„")
    print("=" * 50)
    try:
        all_keywords = df['keywords_list'].explode().tolist()
        top_20_keywords = Counter(all_keywords).most_common(20)
        print("\n ê°€ìž¥ ë¹ˆë²ˆí•˜ê²Œ ë“±ìž¥í•˜ëŠ” í‚¤ì›Œë“œ (ìƒìœ„ 20ê°œ):")
        print(pd.DataFrame(top_20_keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„']))

        title_words = df['Title_ko'].str.split().explode().tolist()
        top_10_title_words = Counter(title_words).most_common(10)
        print("\nðŸ· ì§ë¬´ëª…ì— ê°€ìž¥ ë§Žì´ ì‚¬ìš©ëœ ë‹¨ì–´ (ìƒìœ„ 10ê°œ):")
        print(pd.DataFrame(top_10_title_words, columns=['ì§ë¬´ëª… ë‹¨ì–´', 'ë¹ˆë„']))
    except Exception as e:
        print(f"ë¶„ì„ 1 ì˜¤ë¥˜: {e}")

    # ==========================================================================
    #  ë¶„ì„ 2: ì§ë¬´ íŠ¹ì„± íƒœê¹…
    # ==========================================================================
    print("\n" + "=" * 50)
    print(" ë¶„ì„ 2: ì§ë¬´ íŠ¹ì„± íƒœê¹…")
    print("=" * 50)
    try:
        characteristic_dict = {
            '#ì €ê°•ë„_ì‹ ì²´í™œë™': ['ìƒë‹´', 'ì•ˆë‚´', 'ì‚¬ë¬´', 'ë§ë²—', 'ì‹¤ë‚´ ê·¼ë¬´', 'ê´€ëžŒ'],
            '#ê³ ê°•ë„_ì‹ ì²´í™œë™': ['ìˆœì°°', 'ë°°ë‹¬', 'ì²­ì†Œ', 'ìœ¡ì²´ í™œë™', 'ë¯¸í™”', 'ì¡°ë¦¬', 'ì •ë¦¬'],
            '#ì‹¤ì™¸ê·¼ë¬´': ['ìˆœì°°', 'ì™¸ë¶€ í™œë™', 'êµí†µ ì•ˆë‚´', 'í™˜ê²½ë¯¸í™”'],
            '#ì‹¤ë‚´ê·¼ë¬´': ['ì‹¤ë‚´ ê·¼ë¬´', 'ì‚¬ë¬´', 'ìƒë‹´', 'ëŒë´„', 'ë§¤ìž¥'],
            '#ì‚¬íšŒí™œë™ì„±': ['ìƒë‹´', 'ì•ˆë‚´', 'ë§ë²—', 'ëŒë´„', 'êµìœ¡'],
            '#ë…ë¦½ì ì—…ë¬´': ['ìˆœì°°', 'ì •ë¦¬', 'ë‹¨ìˆœ í¬ìž¥', 'ë³´ì•ˆ']
        }


        def create_tags(keywords):
            tags = []
            # keywordsê°€ ë¬¸ìžì—´ì´ ì•„ë‹Œ ê²½ìš°(ì˜ˆ: float íƒ€ìž…ì˜ NaN)ë¥¼ ëŒ€ë¹„
            if not isinstance(keywords, str): return ''
            for tag, keyword_list in characteristic_dict.items():
                if any(keyword in keywords for keyword in keyword_list):
                    tags.append(tag)
            return ', '.join(tags)


        df['job_tags'] = df['keywords_bert'].apply(create_tags)
        print("\nâœ¨ ì§ë¬´ íƒœê¹… ê²°ê³¼ (ìƒ˜í”Œ):")
        print(df[['Title_ko', 'job_tags']].head())
    except Exception as e:
        print(f"ë¶„ì„ 2 ì˜¤ë¥˜: {e}")

    # ==========================================================================
    #  ë¶„ì„ 3: í† í”½ ëª¨ë¸ë§ì„ í†µí•œ ì§ë¬´ êµ°ì§‘í™”
    # ==========================================================================
    print("\n" + "=" * 50)
    print(" ë¶„ì„ 3: í† í”½ ëª¨ë¸ë§ì„ í†µí•œ ì§ë¬´ êµ°ì§‘í™”")
    print("=" * 50)
    try:
        # 'keywords_bert'ì— NaN ê°’ì´ ìžˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ë¬¸ìžì—´ë¡œ ë³€í™˜
        df['keywords_bert'].fillna('', inplace=True)
        vectorizer = CountVectorizer()
        dtm = vectorizer.fit_transform(df['keywords_bert'])
        num_topics = 5
        lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
        lda.fit(dtm)

        print(f"\nðŸ” ë°œê²¬ëœ ì§ë¬´ êµ°ì§‘ (í† í”½):")
        feature_names = vectorizer.get_feature_names_out()
        for topic_idx, topic in enumerate(lda.components_):
            top_keywords = " ".join([feature_names[i] for i in topic.argsort()[:-8:-1]])
            print(f"ðŸ“‚ êµ°ì§‘ #{topic_idx + 1}: {top_keywords}")
    except Exception as e:
        print(f"ë¶„ì„ 3 ì˜¤ë¥˜: {e}")

    # ==========================================================================
    #  ë¶„ì„ 4: ìž‘ì—… êµ°ì§‘ì— ëŒ€í•œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
    # ==========================================================================
    print("\n" + "=" * 50)
    print(" ë¶„ì„ 4: ìž‘ì—… êµ°ì§‘ì— ëŒ€í•œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
    print("=" * 50)
    try:
        keyword_pairs = [pair for keywords in df['keywords_list'] if isinstance(keywords, list) for pair in
                         combinations(sorted(keywords), 2)]

        if keyword_pairs:
            pair_counts = Counter(keyword_pairs)
            print("\nðŸ”— ê°€ìž¥ í”í•œ ìž‘ì—… ìŒ (ìƒìœ„ 10ê°œ):")
            print(pair_counts.most_common(10))

            G = nx.Graph()
            for pair, count in pair_counts.most_common(30):
                G.add_edge(pair[0], pair[1], weight=count)

            if G.nodes():
                plt.figure(figsize=(16, 12))
                pos = nx.spring_layout(G, k=0.9, iterations=50)
                d = dict(G.degree)


                nx.draw(G, pos, with_labels=True, node_color='skyblue',
                        node_size=[v * 100 for v in d.values()],
                        font_size=12, edge_color='gray', alpha=0.8,
                        font_family=font_name)

                plt.title('ìž‘ì—… êµ°ì§‘ ë„¤íŠ¸ì›Œí¬ ê·¸ëž˜í”„', size=20)
                plt.savefig('task_network.png')
                print("\nâœ… ë„¤íŠ¸ì›Œí¬ ê·¸ëž˜í”„ê°€ 'task_network.png'ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâš ï¸ ë¶„ì„í•  í‚¤ì›Œë“œ ìŒì´ ì—†ì–´ ë„¤íŠ¸ì›Œí¬ ê·¸ëž˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ë¶„ì„ 4 ì˜¤ë¥˜: {e}")

print("\nëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")