import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from collections import Counter
from itertools import combinations
import networkx as nx
import warnings

#ì§ë¬´ë°ì´í„°ë¶„ì„

# ì‚¬ì†Œí•œ ê²½ê³  ë©”ì‹œì§€ëŠ” ë¬´ì‹œ
warnings.filterwarnings('ignore')

# ---  ì‹œê°í™” ìœ„í•œ í•œê¸€ í°íŠ¸ ì„¤ì • ---
# ì‚¬ìš©ì ìš”ì²­ ë° ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ í°íŠ¸ ì„¤ì • ë¶€ë¶„ì„ ì£¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
# ê·¸ë˜í”„ì˜ í•œê¸€ì´ ê¹¨ì ¸ ë³´ì¼ ìˆ˜ ìˆìœ¼ë‚˜, ë¶„ì„ ìì²´ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
# try:
#     font_path = None
#     for font in fm.fontManager.ttflist:
#         if 'Nanum' in font.name or 'Malgun' in font.name:
#             font_path = font.get_file()
#             break
#     if font_path:
#         font_prop = fm.FontProperties(fname=font_path)
#         plt.rcParams['font.family'] = font_prop.get_name()
#         print("âœ… í•œê¸€ í°íŠ¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
#     else:
#         print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œê°í™” ì‹œ ê¸€ìê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
#     plt.rcParams['axes.unicode_minus'] = False
# except Exception as e:
#     print(f"í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ---  ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬ ---
file_path = 'job_code.csv'
try:
    # â­ï¸ í•µì‹¬ ìˆ˜ì • ì‚¬í•­: í•œê¸€ CSV íŒŒì¼ì´ ê¹¨ì§ˆ ë•Œ 'cp949' ì¸ì½”ë”©ì„ ì§€ì •í•˜ì—¬ í•´ê²°í•©ë‹ˆë‹¤.
    df = pd.read_csv(file_path, encoding='cp949')
    print(f"\n '{file_path}' íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

    # 'keywords_bert'ê°€ ë¹„ì–´ìˆëŠ” í–‰ì€ ë¶„ì„í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤
    df.dropna(subset=['keywords_bert'], inplace=True)
    # ì²˜ë¦¬ë¥¼ ì‰½ê²Œ í•˜ê¸° ìœ„í•´ í‚¤ì›Œë“œë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤
    df['keywords_list'] = df['keywords_bert'].str.split(', ')

    print("\n---------- [ ë°ì´í„° ì •ë³´ ] ----------")
    df.info()
    print("\n---------- [ ë°ì´í„° ìƒ˜í”Œ ] ----------")
    print(df.head())

except FileNotFoundError:
    print(f" ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ìœ„ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    df = pd.DataFrame()  # ìŠ¤í¬ë¦½íŠ¸ê°€ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ ë¹ˆ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤
except UnicodeDecodeError:
    print(f" ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì˜ ì¸ì½”ë”© ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'utf-8'ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
        print(f"\n '{file_path}' íŒŒì¼ì„ 'utf-8'ë¡œ ë‹¤ì‹œ ì‹œë„í•˜ì—¬ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f" ì˜¤ë¥˜: íŒŒì¼ ì½ê¸°ì— ìµœì¢…ì ìœ¼ë¡œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ì¸: {e}")
        df = pd.DataFrame()


# --- ë©”ì¸ ë¶„ì„ ë¸”ë¡ (ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰) ---
if not df.empty:

    # ==========================================================================
    #  ë¶„ì„ 1: ì „ì²´ í‚¤ì›Œë“œ ë° ì§ë¬´ëª… ë¶„ì„
    # ==========================================================================
    print("\n" + "=" * 50)
    print(" ë¶„ì„ 1: ì „ì²´ í‚¤ì›Œë“œ ë° ì§ë¬´ëª… ë¶„ì„")
    print("=" * 50)
    try:
        all_keywords = df['keywords_list'].explode().tolist()
        top_20_keywords = Counter(all_keywords).most_common(20)
        print("\n ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ (ìƒìœ„ 20ê°œ):")
        print(pd.DataFrame(top_20_keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„']))

        title_words = df['Title_ko'].str.split().explode().tolist()
        top_10_title_words = Counter(title_words).most_common(10)
        print("\nğŸ· ì§ë¬´ëª…ì— ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ (ìƒìœ„ 10ê°œ):")
        print(pd.DataFrame(top_10_title_words, columns=['ì§ë¬´ëª… ë‹¨ì–´', 'ë¹ˆë„']))
    except Exception as e:
        print(f"ë¶„ì„ 1 ì˜¤ë¥˜: {e}")

    # ==========================================================================
    #  ë¶„ì„ 2: ì§ë¬´ íŠ¹ì„± íƒœê¹…
    # ==========================================================================
    print("\n" + "=" * 50)
    print(" ë¶„ì„ 2: ì§ë¬´ íŠ¹ì„± íƒœê¹…")
    print("=" * 50)
    try:
        characteristic_dict = {
            '#ì €ê°•ë„_ì‹ ì²´í™œë™': ['ìƒë‹´', 'ì•ˆë‚´', 'ì‚¬ë¬´', 'ë§ë²—', 'ì‹¤ë‚´ ê·¼ë¬´', 'ê´€ëŒ'],
            '#ê³ ê°•ë„_ì‹ ì²´í™œë™': ['ìˆœì°°', 'ë°°ë‹¬', 'ì²­ì†Œ', 'ìœ¡ì²´ í™œë™', 'ë¯¸í™”', 'ì¡°ë¦¬', 'ì •ë¦¬'],
            '#ì‹¤ì™¸ê·¼ë¬´': ['ìˆœì°°', 'ì™¸ë¶€ í™œë™', 'êµí†µ ì•ˆë‚´', 'í™˜ê²½ë¯¸í™”'],
            '#ì‹¤ë‚´ê·¼ë¬´': ['ì‹¤ë‚´ ê·¼ë¬´', 'ì‚¬ë¬´', 'ìƒë‹´', 'ëŒë´„', 'ë§¤ì¥'],
            '#ì‚¬íšŒí™œë™ì„±': ['ìƒë‹´', 'ì•ˆë‚´', 'ë§ë²—', 'ëŒë´„', 'êµìœ¡'],
            '#ë…ë¦½ì ì—…ë¬´': ['ìˆœì°°', 'ì •ë¦¬', 'ë‹¨ìˆœ í¬ì¥', 'ë³´ì•ˆ']
        }

        def create_tags(keywords):
            tags = []
            if not isinstance(keywords, str): return ''
            for tag, keyword_list in characteristic_dict.items():
                if any(keyword in keywords for keyword in keyword_list):
                    tags.append(tag)
            return ', '.join(tags)

        df['job_tags'] = df['keywords_bert'].apply(create_tags)
        print("\nâœ¨ ì§ë¬´ íƒœê¹… ê²°ê³¼ (ìƒ˜í”Œ):")
        print(df[['Title_ko', 'job_tags']].head())
    except Exception as e:
        print(f"ë¶„ì„ 2 ì˜¤ë¥˜: {e}")

    # ==========================================================================
    #  ë¶„ì„ 3: í† í”½ ëª¨ë¸ë§ì„ í†µí•œ ì§ë¬´ êµ°ì§‘í™”
    # ==========================================================================
    print("\n" + "=" * 50)
    print(" ë¶„ì„ 3: í† í”½ ëª¨ë¸ë§ì„ í†µí•œ ì§ë¬´ êµ°ì§‘í™”")
    print("=" * 50)
    try:
        vectorizer = CountVectorizer()
        dtm = vectorizer.fit_transform(df['keywords_bert'])
        num_topics = 5
        lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
        lda.fit(dtm)

        print(f"\nğŸ” ë°œê²¬ëœ ì§ë¬´ êµ°ì§‘ (í† í”½):")
        feature_names = vectorizer.get_feature_names_out()
        for topic_idx, topic in enumerate(lda.components_):
            top_keywords = " ".join([feature_names[i] for i in topic.argsort()[:-8:-1]])
            print(f"ğŸ“‚ êµ°ì§‘ #{topic_idx + 1}: {top_keywords}")
    except Exception as e:
        print(f"ë¶„ì„ 3 ì˜¤ë¥˜: {e}")

    # ==========================================================================
    #  4: ì‘ì—… êµ°ì§‘ì— ëŒ€í•œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
    # ==========================================================================
    print("\n" + "=" * 50)
    print(" ë¶„ì„ 4: ì‘ì—… êµ°ì§‘ì— ëŒ€í•œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
    print("=" * 50)
    try:
        keyword_pairs = [pair for keywords in df['keywords_list'] if isinstance(keywords, list) for pair in
                         combinations(sorted(keywords), 2)]

        if keyword_pairs:
            pair_counts = Counter(keyword_pairs)
            print("\nğŸ”— ê°€ì¥ í”í•œ ì‘ì—… ìŒ (ìƒìœ„ 10ê°œ):")
            print(pair_counts.most_common(10))
       # ì‹œê°í™” ì£¼ì„ì²˜ë¦¬
        #     G = nx.Graph()
        #     for pair, count in pair_counts.most_common(30):
        #         G.add_edge(pair[0], pair[1], weight=count)
        #     if G.nodes():
        #         plt.figure(figsize=(16, 12))
        #         pos = nx.spring_layout(G, k=0.9, iterations=50)
        #         d = dict(G.degree)
        #         nx.draw(G, pos, with_labels=True, node_color='skyblue',
        #                 node_size=[v * 100 for v in d.values()],
        #                 font_size=12, edge_color='gray', alpha=0.8)
        #         plt.title('ì‘ì—… êµ°ì§‘ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„', size=20)
        #         plt.savefig('task_network.png')
        #         print("\nâœ… ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ê°€ 'task_network.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ë¶„ì„ 4 ì˜¤ë¥˜: {e}")

print("\n ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œ.")